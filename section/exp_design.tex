\section{设计思想}

\subsection{PPO算法}

\imagefigure{dev_bp.png}{dev_bp}{代码结构及功能}

在智能体决策1V1实验中，环境产生的原始观测数据不能直接作为智能体模型的输入，并且不同的用户开发的智能体一般是不一样的，显然不同的智能体的决策、学习方法的输入输出也是不一样的，所以应该先定义智能体模型输入输出的数据结构。包括特征（ObsData）、动作（ActData）、样本（SampleData），其中ObsData和ActData分别作为智能体predict方法的输入和输出，SampleData作为智能体learn方法的输入。
此外需要实现一个智能体，能够处理ObsData和ActData类型，由于可能会定义不同的数据结构，同时环境接口输入输出的数据结构是固定的，因此环境接口的输入输出数据和智能体接口的输入输出数据需要进行转换，所以还需要实现这些数据结构的转换方法，包括：observation\_process和action\_process。
此外，智能体还需要包含一个模型（一般是神经网络模型），智能体负责与环境交互，产生预测动作并消费样本来训练模型。在实现了\ 数据结构，数据处理函数，模型和\ 智能体\ 以及回报设计等之后，还需要实现一个强化学习的训练流程workflow，将所有组件组合起来完成强化学习训练，即智能体通过不断的与环境交互，获取样本数据，更新并迭代模型，直到模型收敛到期望的效果。平台中提供的代码框架各部分功能如 \cref{fig:dev_bp} 所示。

\imagefigure{dev_train.png}{dev_train}{分布式训练架构}

开悟平台采用分布式训练，在训练开始后启动一个样本池，一个模型同步服务。分布式训练架构如 \cref{fig:dev_train} 所示。学习容器会定期保存模型，agent.learn(samples)调用将会把样本发送到样本池，训练容器会从样本池中采样样本samples将其传入agent.learn(samples)进行训练，此过程是自动的，用户无需开发额外代码。
智能体决策1V1实验虽然提供了两个智能体，但由于在selfplay模式下红蓝双方的两个智能体是同构的，所以只需训练一个模型，即训练容器（learner）中仅有一个模型实例，所以样本池也只有一个，两个智能体调用agent.learn()将发送样本到同一个样本池；同样地，模型同步服务也只同步一个模型，用户可以按需在恰当时机从模型同步服务加载模型。用户可以为两个智能体加载不同的历史模型，调用agent.load\_model(id="latest")将会加载最新模型，若希望加载随机模型则调用agent.load\_model(id="random")，若训练过程希望加载某个模型用于评估则可以指定模型id。


环境返回的观测信息obs对应与PPO算法中的状态s。开悟平台的实验环境中已经对这部分数据做了预处理，但是智能体需要的状态信息s和环境观测信息obs之间存在一定差异，这部分由observation\_process函数将环境提供的观测信息转换为智能体需要的状态信息。同理，action\_process函数用于将智能体选择的动作转换为环境可以识别的动作。sample\_process用于将环境交互数据和奖励打包成样本数据，用于经验回放。在分布式训练中，样本数据需要通过网络传输至样本池，因此需要对SampleData进行序列化和反序列化，此处应该注意序列化编码和解码的对应，防止数据出错。

\subsection{PPO截断}

\subsection{奖励函数}

\subsubsection{状态检测}
\subsubsection{经济奖励}
\subsubsection{经验奖励}
\subsubsection{技能使用条件与奖励}
\subsubsection{获胜奖励}
\subsubsection{惩罚}