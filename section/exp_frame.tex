\section{实验环境及实验要求}
本次实验在腾讯开悟教学平台中进行。开悟平台中提供了机器学习所需硬件平台，基于游戏王者荣耀建立的智能体模拟环境，网页版的代码开发调试环境，以及训练管理和评估环境。

\subsection{实验环境描述}
本实验通过算法训练一个智能体，在地图获取经济和经验来提高英雄的攻击力，率先摧毁对方的水晶以获得胜利。若己方水晶被摧毁，则对方获得胜利。

\subsubsection{问题描述}
腾讯开悟教学平台中的智能体决策1V1实验，基于游戏王者荣耀建立，问题要求通过算法训练一个智能体，让其在游戏回合中学习游戏操纵策略，通过击杀对方角色、士兵和野怪积累经济和经验，拆毁水晶塔取得胜利。其中经济用于购买角色装备，提升攻击力和抗性；经验用于提升角色等级，增加角色攻击力和抗性，以及提高技能的参数。

\imagefigure{bg.png}{background}{地图}

智能体决策1V1的地图如 \cref{fig:background} 所示。地图主体为墨家机关直道，从左下到右上建筑依次为：我方泉水法阵，我方水晶，我方防御塔，中央战场，敌方防御塔，敌方水晶，敌方泉水法阵。其中泉水法阵为英雄出生和复活地点，泉水法阵回定期刷新双方士兵。防御塔会攻击进入一定范围内的敌方阵容，防御塔下会生成治疗符文，英雄拾取治疗符文后回复一定量的生命值，治疗符文被拾取后经过固定的冷却时间刷新。治疗符文不区分阵容，治疗符文未拾取时不会生成多个。中央广场的丛林中会定期刷新丛林野怪，丛林野怪没有阵营。丛林野怪会跟随并攻击最后一个攻击自己的角色，丛林野怪有固定的巢穴范围，跟随攻击目标不会超出巢穴范围并且有最大跟随间距，超出巢穴范围或超出最大跟随间距时放弃跟随并返回初始位置，同时清除对攻击目标的记忆。游戏中需要拆毁防御塔之后的水晶取得胜利。实验中进行了简化游戏回合，在拆毁对方防御塔后判定取胜。


实验平台中提供三个英雄角色，每个英雄角色有一个被动技能和三个不同的主动技能，被动技能会自动触发，主动技能需要智能体采取动作才会触发。不同英雄的技能不同，此外每个英雄还有通用的闪现技和召回技能，以及最基础的普通攻击和移动。

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|l|} \hline
        \bf 技能 & \bf 描述                                             \\ \hline
        空       & 未定义                                               \\ \hline
        空       & 未定义                                               \\ \hline
        移动     & 移动英雄                                             \\ \hline
        普通攻击 & 释放普通攻击                                         \\ \hline
        技能一   & 释放英雄技能一                                       \\ \hline
        技能二   & 释放英雄技能二                                       \\ \hline
        技能三   & 释放英雄技能三                                       \\ \hline
        治疗技能 & 释放治疗技能                                         \\ \hline
        自选技能 & 释放自选技能：闪现                                   \\ \hline
        召回     & 经过一定时间后传送英雄回到泉水，攻击或者操作会被打断 \\ \hline
        技能四   & 释放技能四，英雄技能需要的辅助操作                   \\ \hline
        装备技能 & 释放特定装备提供的技能                               \\ \hline
    \end{tabular}
    \caption{技能列表} \label{tab:actions}
\end{table}

\imagefigure[0.5]{skill_location.png}{skillloc}{技能释放方位的计算}

在智能体决策1V1实验中，英雄的动作被分成三个维度来定义：种类，方位，目标。动作种类包括：移动，攻击，三个英雄主动技能，两个通用技能闪现和召回，其定义如 \cref{tab:actions} 所示。其次是动作方位，包括移动方位和技能释放方位。动作方位是否有效取决于选定的动作，需要按照动作类型进行过滤。通常动作方位由方向和强度两部分组成，移动方位直接用16x16的矩阵在直角坐标中表示，技能释放方位用目标和16x16的偏移矩阵表示，如 \cref{fig:skillloc} 所示。最后是动作目标，单体的攻击和治疗动作通常仅需要指定对象，而范围技能的方位依赖与动作目标和偏移。

\imagefigure{base.png}{base}{游戏画面}

游戏回合中，视野是跟随英雄移动的，如 \cref{fig:base} 所示，双方英雄看到的视野不同。游戏引擎会返回当前帧的状态数据，从中可以获取到英雄血量、技能信息、防御塔信息等数值，如 \cref{fig:base} 所示。观测到的特征经过视野过滤, 观测数据中仅提供英雄视野中的数据，当敌方英雄不在视野中时, 敌方英雄部分特征会被置为默认值。环境中观测到的特征信息如 \cref{tab:frame_state} 所示。主要包括英雄状态、非英雄角色状态、子弹装备物品等信息。

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|l|} \hline
        字段名        & 字段类型            & 备注                 \\ \hline
        frameNo       & signed int          & 帧号                 \\ \hline
        hero\_states  & vector, HeroState   & 英雄状态列表         \\ \hline
        npc\_states   & vector, ActorState  & 非英雄角色状态列表   \\ \hline
        bullets       & vector, Bullet      & 子弹列表             \\ \hline
        cakes         & vector, Cake        & 功能物件列表(血包等) \\ \hline
        equip\_infos  & vector, EquipInfo   & 装备信息列表         \\ \hline
        frame\_action & vector, FrameAction & 死亡事件             \\ \hline
    \end{tabular}
    \caption{状态信息} \label{tab:frame_state}
\end{table}


智能体决策1V1实验中游戏回合的目标是摧毁敌方水晶(实际设定是防御塔)，除此之外不计算任何得分，游戏结果只有胜负，最终结果按照胜负比例计算，超时不计算胜负。游戏回合中击杀对方阵容角色或者丛林野怪，通过提升己方攻击能力和削弱对方攻击能力，来间接的影响胜负。

\subsubsection{环境配置}

智能体决策1V1实验中需要配置的信息包括敌方模型和双方英雄。为了统一，通常选定蓝方进行算法的训练和评估，并进行过程监控。对局模式中的红方，可以指定基础AI模型或者其他特定的智能体模型，也可以指定为自对局模式，即红蓝双方都使用同一个智能体进行训练和评估。每个算法都可以自定义环境配置，通过 \verb|user_conf| 字典实现，格式为：

%\begin{noindent}
{
\linespread{1.25} \selectfont
\begin{lstlisting}[language=Python]
usr_conf = {
    "diy": {
        "monitor_side": 0,
        "monitor_label": "common_ai",
        "lineups": [[{"hero_id": 133}],[{"hero_id": 508}]],
        }
    }
\end{lstlisting}
}
%\end{noindent}

配置字典中各个字段信息解释如下：

\begin{enumerate}
    \item monitor\_side : 选择的阵营，统计指标以该阵营为第一视角，0为蓝方阵营，1为红方阵营
    \item  monitor\_label : 对手配置，三种类型:
          \begin{enumerate}
              \item "selfplay"，自对弈
              \item "common\_ai"，对手采用common\_ai
              \item 自定义的模型id，如“1234”，对手选用指定模型
          \end{enumerate}
    \item lineups : 阵容配置，分别代表蓝方和红方英雄id，hero\_id范围：\verb|[133, 199, 508]|
\end{enumerate}

由于智能体决策1V1的评估中红蓝双方的英雄都仅支持133狄仁杰，因此将monitor\_side为0，同时设置lineups为\verb|[[{hero_id:133}], [{hero_id:133}]]|。在训练中仅需要修改monitor\_label来选择对局的对手模型即可。

\subsubsection{代码框架}

智能体决策1V1实验提供了基础的代码框架，包含已经实现的PPO算法，以及一个可以自己实现的diy算法。以PPO算法为例，各代码文件作用为：
\begin{center}
    \begin{tabular}{|c|l|} \hline
        \bfseries 文件              & \bfseries 用途         \\ \hline
        train\_test                 & 调试入口               \\ \hline
        ppo/config                  & 算法配置文件           \\ \hline
        ppo/train\_workflow         & 算法流程               \\ \hline
        ppo/algorithm/agent         & 智能体实现             \\ \hline
        ppo/feature/definition      & 状态观测和状态变化检测 \\ \hline
        ppo/feature/reward\_manager & 计算奖励               \\ \hline
        ppo/model/model             & 神经网络模型           \\ \hline
    \end{tabular}
\end{center}

其他与环境交互的代码包被隐藏防止修改，以保证游戏回合的公平性。实验者无需关心和环境交互的实现部分。

\imagefigure[1.0]{train_flow.png}{train_flow}{智能体训练流程}

实验中的智能体包含一个可被训练的模型，智能体可以对环境给出的观测进行决策，这个决策作用于环境产生新的观测，此过程通过训练流程控制，不断循环。训练流程还要收集循环过程中产生的每一帧数据，将他们组合成样本数据，智能体可以根据这些样本作为算法的输入，通过算法更新模型。由于Hok1v1采用分布式训练，会启动多个容器，样本需要通过网络通信发送到训练容器（learner）中进行训练，所以需要对样本进行编码方便网络发送，另外智能体需要将learner容器上的模型同步回来。以上任务描述如 \cref{fig:train_flow} 所示。


平台上提供的代码框架中已经实现了一个版本的PPO算法，并且可以运行训练得到结果。基于已有的PPO算法，可以在diy目录中自己实现一个智能体，然后进行训练和评估。由于平台的限制，代码目录中仅能识别ppo和diy算法目录，不支持新增或自定义其他算法实现。

\subsubsection{模型训练}

\imagefigure[1.0]{train_manage.png}{train_manage}{训练管理界面}
模型训练在实验的控制台主页上，通过左侧菜单选择训练管理标签进入。在代码修改并测试结束后，通过新增训练任务，使用最新的代码进行模型训练。训练管理界面如 \cref{fig:train_manage} 所示。

\imagefigure[0.5]{train_add.png}{train_add}{新增训练任务}
在新增训练任务的界面可以选择使用的算法实现。智能体决策1V1实验的框架支持加载已经训练好的模型继续进行训练。最长训练时间为10小时，需要合理的设计网络模型，调整学习率和奖励函数，注意模型的收敛时间。新增训练任务的界面如 \cref{fig:train_add} 所示，可以选择从头开始训练或者从一个基础模型开始训练。

\imagefigure[1.0]{monitor.png}{monitor}{训练监控界面}
模型的训练过程可以提前中止。模型训练过程中每隔一定时间会保存一个模型文件。在模型训练的监控界面，可以看到模型训练过程中的数据变化，监控模型是否收敛以及收敛速度是否符合预期，可以提前中止模型训练以节省时间。训练监控界面如 \cref{fig:monitor} 所示。默认配置的模型保存时间为30分钟。初始训练时间较长，模型的保存时间间隔可以较长防止占用过多的磁盘空间。后期基于已有模型进行训练时，训练时间较短，可以采用较短的保存间隔。


\imagefigure{train_result.png}{train_result}{训练过程模型列表}
在模型训练任务的模型列表界面可以看到训练过程中保存的所有模型，可以根据监控数据找到对应的模型，并提交到模型管理中以备后用，防止模型被清理。模型列表如 \cref{fig:train_result} 所示。

\subsubsection{模型管理}

\imagefigure[1.0]{model_manage.png}{model_manage}{模型管理界面}
模型管理菜单中显示所有已经提交至模型管理中的模型，提交到模型管理中可以保证模型不会被清理。模型管理中的模型可以进行评估和下载操作。模型提交到模型管理中后，首先会进行检测，检测成功后才可以进行评估。模型管理界面如 \cref{fig:model_manage} 所示。

\subsubsection{模型评估}

\imagefigure[1.0]{model_eval.png}{model_eval}{模型评估管理界面}

模型评估是对已经训练结束的模型运行回合查看结果。评估任务提交后会进行排队等待运行，运行结束后可以查看结果。模型评估任务管理界面如 \cref{fig:model_eval} 所示。

\imagefigure[0.5]{eval_add.png}{eval_add}{添加模型评估任务}

添加模型评估任务界面如 \cref{fig:eval_add} 所示。添加模型评估任务时可以配置的参数包括：任务名称，双方英雄，双方智能体模型，对战回合数量。其中，英雄只能选择狄仁杰，双方智能体模型可以选择模型管理中的任意模型，不支持 common\_ai 模型。智能体决策1V1实验中不提供通用或者公用的模型，因此找到一个合理的评估基准，此处选用原始版本代码训练得到的模型作为基准。评估会作为红蓝阵营分别进行，最大选择评估回合数为5，则作为红蓝阵营分别进行5个回合，总计10个游戏回合。由于地图的对称性，理论上作为红蓝双方阵营应该是没有区别的。

\imagefigure[0.6]{eval_info.png}{eval_info}{模型评估任务配置}
在模型评估管理界面看可以看到评估任务的具体配置信息，如 \cref{fig:eval_info} 所示。包含对战双方选择的模型信息和英雄信息。

\imagefigure{eval_detail.png}{eval_detail}{模型评估结果列表}
在模型评估结果的详情页面中，可以看到本次评估任务的具体信息。如 \cref{fig:eval_detail} 所示。该页面展示了游戏回合中通常需要关注的信息，如经验、经济、击杀和死亡等信息。需要注意的是，在智能体决策1V1的实验中，智能体操纵的英雄发育成长的优劣通常通过经验、经济、击杀和死亡这些信息来评估，但游戏的胜负是按照拆毁水晶计算。英雄的发育成长为拆毁水晶提供了基础。

\imagefigure{eval_preview.png}{eval_preview}{模型评估游戏回合快速预览}
在模型评估任务的详情页面，可以观看游戏回合的快速预览。预览采用俯视图的模式，显示完整的游戏地图，如 \cref{fig:eval_preview} 所示。其中红色和蓝色的点代表双方阵营的士兵和英雄，较大的点代表英雄。快速预览中可以观察到英雄和士兵在地图中的位置分布。更具体的游戏信息可以通过下载录像的方式进行查看。

\imagefigure[0.6]{player.png}{player}{游戏录像播放器}
在模型评估任务的详情页中，可以下载评估任务每个游戏回合的回放。观看游戏回放前需要先下载安装游戏回放的播放器，然后将下载的游戏回放文件复制到auto\_replay文件夹中，执行SGame程序，自动开始回放。下载得到的播放器如 \cref{fig:player} 所示。播放过程中不能控制播放进度和速度。播放完成后会自动删除录像文件，因此在播放完成前可以结束播放器，避免录像文件被删除。游戏录像回放界面如 \cref{fig:base} 所示，可以通过鼠标滚轮缩放视角或者通过按键移动画面中心位置，以查看英雄角色的具体表现。

\subsubsection{分析调试}
开悟平台提供了基于网页版vscode的在线开发和调试环境。通过实验右上角的“进入开发”按钮可以打开代码开发调试环境。代码开发调试环境空闲10分钟会被回收。开发调试环境的操作和标准的vscode基本一致。在开发调试环境中修改代码保存后，就可以到训练管理界面采用最新的代码进行训练。训练开始时会首先对当前的代码打包生成快照，打包过程中会进行文件大小的检测，如果有较大的文件会打包失败。编写代码时应该注意拆分文件保证大小符合要求。


代码开发环境同时还可以进行调试。调试脚本的入口为train\_test.py函数。由于代码中目前无法加载模型，调试时只能加载一个初始化的模型从头进行训练。开发环境中已经配置好了调试的配置文件，从左侧功能栏切换到到调试界面直接运行即可。调试时需要注意，train\_test.py脚本属于每次启动开发环境时通过模板创建，该文件中更改的代码在开发环境关闭后会丢失。每次进入开发环境调试时需要确认采用的算法名称是需要进行调试的算法。调试时在终端区域可以看到日志输出，调试功能和标准vscode基本一致。需要注意开发环境是每次启动时生成的，只有ckpt，conf，diy，ppo，log这几个目录是链接到用户的存储目录中，因此开发环境无法用git进行代码版本管理，修改代码时注意保存。

\subsection{实验要求}
\subsubsection{掌握深度强化学习的基本知识}

传统的强化学习通常使用表格方法或线性函数，适用于状态空间较小、定义明确的问题。深度强化学习通过使用深度神经网络扩展了强化学习，使其能够处理像图像这样的高维度、非结构化输入，并学习更复杂的策略。虽然深度强化学习使用的是深度学习模型，但其学习机制却有本质区别。大多数深度学习应用涉及监督学习（从有标签的数据中学习）或无监督学习（在无标签的数据中寻找模式）。深度强化学习通过互动和反馈（奖励）进行学习，每一步都没有明确的正确操作标签。深度强化学习和监督学习、无监督学习也有区别。监督学习要求数据集具有正确的输入输出对（训练数据）。无监督学习寻求数据的内在结构。深度强化学习在互动环境中运行，通过基于标量奖励信号的探索和利用，学习最佳行为。

\subsubsection{理解基于策略的强化学习方法}
强化学习算法分为基于值和基于策略两种方式。基于策略是指，输入环境状态，直接输出各个动作的概率或动作强度。训练刚开始时，动作概率分布没有明显差异，趋向于随机探索。
策略梯度算法的思想是先将策略表示成一个和奖励有关的连续函数，然后用连续函数的优化方法去寻找最优的策略，优化目标是最大化连续函数，最常用的是优化方法是梯度上升法（与最小化loss的梯度下降相对）。


PPO算法属于策略优化算法的一种改进型。传统方法可能存在训练过程中策略更新过大导致的不稳定性问题，而PPO通过限制策略更新的幅度来解决这一点。PPO算法基于TRPO算法可信区域的思想，但是其算法实现更加简单。大量的实验结果表明，与TRPO相比，PPO能学习得一样好（甚至更快），这使得 PPO 成为非常流行的强化学习算法。如果我们想要尝试在一个新的环境中使用强化学习算法，那么 PPO 就属于可以首先尝试的算法。PPO算法限制学习策略的方式有两种，惩罚和截断，通常选用截断。

\subsubsection{理解并应用PPO算法}
PPO算法是对TRPO算法的改进，TRPO算法是对Actor-Critic算法的优化。PPO算法包含两个网络，策略网络和价值网络。PPO算法的流程描述如下：
\begin{lstlisting}
    初始化策略网络和价值网络
    循环直到收敛：
        收集轨迹数据：使用当前策略与环境交互N步
        计算优势估计：通过GAE方法计算各状态的优势值
        优化阶段（执行K个回合）:
            随机小批量采样数据
            计算新策略概率比
            计算策略损失
            计算价值损失
            计算熵损失
            联合更新
        定期同步旧策略参数
\end{lstlisting}

