\section{实验目的}

强化学习（Reinforcement Learning, RL）是一种机器学习方法，智能体通过与环境的交互来学习如何最大化某种累积奖励的最佳决策。
基于策略的强化学习，其目标是找到一种策略（Policy），使得在环境中的行为能够最大化累积奖励。
策略是一个映射状态到行为的函数，它直接决定了在给定状态下智能体应该采取的行为。


REINFORCE 算法是策略梯度方法的典型代表，智能体根据当前策略和环境交互，通过采样得到的轨迹数据直接计算出策略参数的梯度，进而更新当前策略，使其向最大化期望回报的目标靠近。
基于策略的强化学习比基于价值的强化学习算法的优化目标（一般是时序差分误差的最小化）要更加直接。
REINFORCE 算法理论上是能保证局部最优的，它实际上是借助蒙特卡洛方法采样轨迹来估计动作价值，这种做法的一大优点是可以得到无偏的梯度。
但是，因为使用了蒙特卡洛方法，REINFORCE 算法的梯度估计的方差很大，可能会造成一定程度上的不稳定。


TRPO (Trust Region Policy Optimization) 一种策略梯度方法，它在策略梯度的基础上引入了约束条件，以确保策略的变化在一定范围内并且是单调变化的。
TRPO 的目标是在满足约束条件的前提下，找到能够最大化累积奖励的策略。TRPO 方法通过对策略梯度进行优化，以实现策略的更新。


PPO (Proximal Policy Optimization) 是 TRPO 算法的改进版。
PPO主要有两种形式：一种是PPO-惩罚，通过KL散度的惩罚项来限制更新；另一种是PPO-截断，即通过截断比值的方式来限制策略的变化。
常用的是后者，也就是 PPO-clip，它通过裁剪概率比来防止过大的更新，这样避免了计算二阶导数，用一阶优化方法就能实现，更易于实现和应用。

实验的目的是理解基于策略梯度的强化学习方法，验证 PPO 算法在稳定性和计算速度方面的改进，测试超参数的敏感性，观察移除截断后策略发生的崩溃现象，并通过修改价值回报理解PPO的应用场景和局限性。